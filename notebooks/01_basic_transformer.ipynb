{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Transformer Implementation with JAX and Flax\n",
    "\n",
    "This notebook demonstrates how to implement and train a basic transformer model using JAX and Flax. We'll cover:\n",
    "1. Model setup\n",
    "2. Data preparation\n",
    "3. Training loop\n",
    "4. Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "# Import our model and training utilities\n",
    "from src.models.transformer import SimpleLanguageModel\n",
    "from src.utils.training import create_train_state, train_step, eval_step\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Small Example Dataset\n",
    "\n",
    "For this demonstration, we'll create a tiny synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_dummy_batch(batch_size: int = 4, seq_len: int = 16, vocab_size: int = 1000):\n",
    "    \"\"\"Creates a dummy batch of data for testing.\"\"\"\n",
    "    return {\n",
    "        'input_ids': jnp.randint(0, vocab_size, (batch_size, seq_len)),\n",
    "        'labels': jnp.randint(0, vocab_size, (batch_size, seq_len)),\n",
    "        'attention_mask': jnp.ones((batch_size, seq_len))\n",
    "    }\n",
    "\n",
    "# Create example batch\n",
    "batch = create_dummy_batch()\n",
    "print(\"Input shape:\", batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Training State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': 1000,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 4,\n",
    "    'mlp_dim': 512,\n",
    "    'dropout_rate': 0.1\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = SimpleLanguageModel(**config)\n",
    "\n",
    "# Initialize training state\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(\n",
    "    rng=rng,\n",
    "    model=model,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's run a few training steps to demonstrate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training loop\n",
    "num_steps = 10\n",
    "dropout_rng = jax.random.PRNGKey(1)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch = create_dummy_batch()\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "    \n",
    "    state, metrics = train_step(state, batch, dropout_rng)\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}: loss = {metrics['loss']:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "eval_batch = create_dummy_batch()\n",
    "eval_metrics = eval_step(state, eval_batch)\n",
    "print(f\"\\nEval loss: {eval_metrics['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "Our training is already optimized with:\n",
    "1. JIT compilation (@jax.jit)\n",
    "2. Efficient memory usage\n",
    "3. GPU/TPU support\n",
    "\n",
    "For multi-device training, we could use @jax.pmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of pmap (only runs if multiple devices are available)\n",
    "if len(jax.devices()) > 1:\n",
    "    print(\"Multiple devices detected, demonstrating pmap...\")\n",
    "    \n",
    "    # Replicate state across devices\n",
    "    state = flax.jax_utils.replicate(state)\n",
    "    \n",
    "    # Define pmapped training step\n",
    "    p_train_step = jax.pmap(train_step, axis_name='batch')\n",
    "    \n",
    "    # Create larger batch for multiple devices\n",
    "    batch = create_dummy_batch(batch_size=8)  # Will be split across devices\n",
    "    \n",
    "    # Run parallel training step\n",
    "    state, metrics = p_train_step(state, batch, dropout_rng)\n",
    "    print(\"Parallel training step completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
