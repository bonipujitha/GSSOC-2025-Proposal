{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques for Training LLMs\n",
    "\n",
    "This notebook demonstrates advanced optimization techniques for training Large Language Models using JAX and Flax:\n",
    "\n",
    "1. Gradient Accumulation\n",
    "2. Mixed Precision Training\n",
    "3. Model Parallelism\n",
    "4. Memory Efficient Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from typing import Any, Tuple\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Accumulation\n",
    "\n",
    "Gradient accumulation allows training with larger effective batch sizes by accumulating gradients over multiple forward/backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_train_state(model, learning_rate, weight_decay):\n",
    "    \"\"\"Initialize training state with optimizer.\"\"\"\n",
    "    params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 8, 64)))\n",
    "    tx = optax.adamw(learning_rate, weight_decay=weight_decay)\n",
    "    return {'params': params, 'opt_state': tx.init(params)}\n",
    "\n",
    "def accumulate_gradients(state, batch, model, n_accumulation_steps):\n",
    "    \"\"\"Accumulate gradients over multiple steps.\"\"\"\n",
    "    def compute_loss(params, x):\n",
    "        logits = model.apply({'params': params}, x)\n",
    "        return jnp.mean((logits - x) ** 2)\n",
    "    \n",
    "    # Split batch into smaller chunks\n",
    "    batch_size = batch.shape[0]\n",
    "    chunk_size = batch_size // n_accumulation_steps\n",
    "    \n",
    "    def accumulate_step(i, grad_acc):\n",
    "        chunk = jax.lax.dynamic_slice(batch, (i * chunk_size, 0, 0),\n",
    "                                     (chunk_size, batch.shape[1], batch.shape[2]))\n",
    "        grad = jax.grad(compute_loss)(state['params'], chunk)\n",
    "        return jax.tree_map(lambda x, y: x + y, grad_acc, grad)\n",
    "    \n",
    "    # Initialize gradient accumulator\n",
    "    grad_acc = jax.tree_map(lambda x: jnp.zeros_like(x), state['params'])\n",
    "    \n",
    "    # Accumulate gradients\n",
    "    grad_acc = jax.lax.fori_loop(\n",
    "        0, n_accumulation_steps,\n",
    "        accumulate_step,\n",
    "        grad_acc\n",
    "    )\n",
    "    \n",
    "    # Average gradients\n",
    "    return jax.tree_map(lambda x: x / n_accumulation_steps, grad_acc)\n",
    "\n",
    "# Example usage\n",
    "model = nn.Dense(64)\n",
    "state = create_train_state(model, 1e-4, 0.01)\n",
    "batch = jax.random.normal(jax.random.PRNGKey(0), (32, 8, 64))\n",
    "accumulated_grads = jax.jit(lambda s, b: accumulate_gradients(s, b, model, 4))(state, batch)\n",
    "print(\"Accumulated gradients shape:\", jax.tree_map(lambda x: x.shape, accumulated_grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixed Precision Training\n",
    "\n",
    "Using mixed precision (float16/bfloat16) can significantly reduce memory usage and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_mp_train_state(model, learning_rate):\n",
    "    \"\"\"Create training state with mixed precision support.\"\"\"\n",
    "    params = model.init(jax.random.PRNGKey(0), \n",
    "                       jnp.ones((1, 8, 64), dtype=jnp.float32))\n",
    "    \n",
    "    # Convert params to bfloat16\n",
    "    mp_params = jax.tree_map(\n",
    "        lambda x: x.astype(jnp.bfloat16) if x.dtype == jnp.float32 else x,\n",
    "        params\n",
    "    )\n",
    "    \n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    return {\n",
    "        'params': mp_params,\n",
    "        'params_fp32': params,  # Keep fp32 copy for optimizer\n",
    "        'opt_state': opt_state\n",
    "    }\n",
    "\n",
    "@jax.jit\n",
    "def mp_train_step(state, batch):\n",
    "    \"\"\"Training step with mixed precision.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        # Convert inputs to bfloat16\n",
    "        x = batch.astype(jnp.bfloat16)\n",
    "        # Forward pass in bfloat16\n",
    "        output = model.apply({'params': params}, x)\n",
    "        # Convert back to float32 for loss computation\n",
    "        return jnp.mean((output.astype(jnp.float32) - batch) ** 2)\n",
    "    \n",
    "    # Compute gradients in mixed precision\n",
    "    grad = jax.grad(loss_fn)(state['params'])\n",
    "    \n",
    "    # Convert gradients back to float32 for optimizer\n",
    "    grad_fp32 = jax.tree_map(lambda x: x.astype(jnp.float32), grad)\n",
    "    \n",
    "    return grad_fp32\n",
    "\n",
    "# Example usage\n",
    "batch = jax.random.normal(jax.random.PRNGKey(0), (16, 8, 64))\n",
    "mp_state = create_mp_train_state(model, 1e-4)\n",
    "mp_grads = mp_train_step(mp_state, batch)\n",
    "print(\"Mixed precision gradients dtype:\", jax.tree_map(lambda x: x.dtype, mp_grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Parallelism\n",
    "\n",
    "Implementing model parallelism for large models across multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ShardedTransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with model parallel attention heads.\"\"\"\n",
    "    num_heads: int\n",
    "    hidden_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Shard the attention heads across devices\n",
    "        def attention_shard(x):\n",
    "            return nn.SelfAttention(\n",
    "                num_heads=self.num_heads // jax.device_count(),\n",
    "                qkv_features=self.hidden_dim\n",
    "            )(x)\n",
    "        \n",
    "        # Parallel attention computation\n",
    "        attention_output = nn.vmap(\n",
    "            attention_shard,\n",
    "            in_axes=0,\n",
    "            out_axes=0,\n",
    "            axis_size=jax.device_count()\n",
    "        )(x)\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "# Example usage (only if multiple devices are available)\n",
    "if len(jax.devices()) > 1:\n",
    "    block = ShardedTransformerBlock(num_heads=8, hidden_dim=64)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(0), (16, 8, 64))\n",
    "    params = block.init(jax.random.PRNGKey(0), x)\n",
    "    \n",
    "    # Shard the computation across devices\n",
    "    sharded_output = jax.pmap(block.apply)(params, x)\n",
    "    print(\"Sharded output shape:\", sharded_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Efficient Attention\n",
    "\n",
    "Implementation of memory-efficient attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def memory_efficient_attention(query, key, value, chunk_size=128):\n",
    "    \"\"\"Memory-efficient attention implementation using chunked computation.\"\"\"\n",
    "    batch_size, seq_len, dim = query.shape\n",
    "    \n",
    "    def chunk_scanner(carry, chunk_idx):\n",
    "        chunk_start = chunk_idx * chunk_size\n",
    "        chunk_end = jnp.minimum(chunk_start + chunk_size, seq_len)\n",
    "        \n",
    "        # Get current chunk of keys and values\n",
    "        k_chunk = jax.lax.dynamic_slice(\n",
    "            key,\n",
    "            (0, chunk_start, 0),\n",
    "            (batch_size, chunk_end - chunk_start, dim)\n",
    "        )\n",
    "        v_chunk = jax.lax.dynamic_slice(\n",
    "            value,\n",
    "            (0, chunk_start, 0),\n",
    "            (batch_size, chunk_end - chunk_start, dim)\n",
    "        )\n",
    "        \n",
    "        # Compute attention scores for this chunk\n",
    "        scores = jnp.matmul(query, k_chunk.transpose(0, 2, 1)) / jnp.sqrt(dim)\n",
    "        chunk_weights = jax.nn.softmax(scores, axis=-1)\n",
    "        chunk_output = jnp.matmul(chunk_weights, v_chunk)\n",
    "        \n",
    "        # Update running sum\n",
    "        new_output = carry + chunk_output\n",
    "        return new_output, None\n",
    "    \n",
    "    # Initialize output with zeros\n",
    "    init_output = jnp.zeros((batch_size, seq_len, dim))\n",
    "    \n",
    "    # Scan over chunks\n",
    "    num_chunks = (seq_len + chunk_size - 1) // chunk_size\n",
    "    final_output, _ = jax.lax.scan(\n",
    "        chunk_scanner,\n",
    "        init_output,\n",
    "        jnp.arange(num_chunks)\n",
    "    )\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Test memory-efficient attention\n",
    "q = jax.random.normal(jax.random.PRNGKey(0), (2, 512, 64))\n",
    "k = jax.random.normal(jax.random.PRNGKey(1), (2, 512, 64))\n",
    "v = jax.random.normal(jax.random.PRNGKey(2), (2, 512, 64))\n",
    "\n",
    "output = memory_efficient_attention(q, k, v)\n",
    "print(\"Memory-efficient attention output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
