{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Attention Mechanisms with JAX and Flax\n",
    "\n",
    "This notebook provides a deep dive into attention mechanisms, a crucial component of transformer models. We'll cover:\n",
    "1. Self-attention implementation\n",
    "2. Multi-head attention\n",
    "3. Visualization of attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Self-Attention\n",
    "\n",
    "Let's implement a basic self-attention mechanism from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simple_self_attention(query, key, value):\n",
    "    \"\"\"Basic self-attention implementation.\n",
    "    \n",
    "    Args:\n",
    "        query: Query vectors [batch_size, seq_len, d_model]\n",
    "        key: Key vectors [batch_size, seq_len, d_model]\n",
    "        value: Value vectors [batch_size, seq_len, d_model]\n",
    "        \n",
    "    Returns:\n",
    "        Attention output and attention weights\n",
    "    \"\"\"\n",
    "    # Compute attention scores\n",
    "    d_k = query.shape[-1]\n",
    "    scores = jnp.matmul(query, key.transpose(0, 2, 1)) / jnp.sqrt(d_k)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "query = jax.random.normal(key, (batch_size, seq_len, d_model))\n",
    "key = jax.random.normal(key, (batch_size, seq_len, d_model))\n",
    "value = jax.random.normal(key, (batch_size, seq_len, d_model))\n",
    "\n",
    "output, weights = simple_self_attention(query, key, value)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention using Flax\n",
    "\n",
    "Now let's implement multi-head attention using Flax's module system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    d_model: int\n",
    "    \n",
    "    def setup(self):\n",
    "        # Head dimension\n",
    "        self.d_k = self.d_model // self.num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Dense(self.d_model)\n",
    "        self.k_proj = nn.Dense(self.d_model)\n",
    "        self.v_proj = nn.Dense(self.d_model)\n",
    "        self.output_proj = nn.Dense(self.d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Linear projections and reshape for multiple heads\n",
    "        q = self.q_proj(x).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        k = self.k_proj(x).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        v = self.v_proj(x).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(self.d_k)\n",
    "        attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "        attention_output = jnp.matmul(attention_weights, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "        return self.output_proj(output), attention_weights\n",
    "\n",
    "# Initialize and test the multi-head attention\n",
    "mha = MultiHeadAttention(num_heads=4, d_model=64)\n",
    "params = mha.init(key, jnp.ones((2, 8, 64)))\n",
    "x = jax.random.normal(key, (2, 8, 64))\n",
    "output, weights = mha.apply(params, x)\n",
    "\n",
    "print(\"Multi-head attention output shape:\", output.shape)\n",
    "print(\"Multi-head attention weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Attention Patterns\n",
    "\n",
    "Let's create a function to visualize attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_attention(attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"Plot attention weights as a heatmap.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_weights, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key position')\n",
    "    plt.ylabel('Query position')\n",
    "    plt.show()\n",
    "\n",
    "# Create some example attention patterns\n",
    "seq_len = 10\n",
    "x = jnp.ones((1, seq_len, 64))\n",
    "output, weights = mha.apply(params, x)\n",
    "\n",
    "# Plot attention weights for the first head\n",
    "plot_attention(weights[0, 0], \"Attention Pattern (Head 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Masked Self-Attention\n",
    "\n",
    "Implementation of masked self-attention, useful for autoregressive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal mask for masked self-attention.\"\"\"\n",
    "    return jnp.triu(jnp.ones((seq_len, seq_len)), k=1) * -1e9\n",
    "\n",
    "def masked_self_attention(query, key, value):\n",
    "    \"\"\"Self-attention with causal masking.\"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    scores = jnp.matmul(query, key.transpose(0, 2, 1)) / jnp.sqrt(d_k)\n",
    "    \n",
    "    # Apply causal mask\n",
    "    mask = create_causal_mask(query.shape[1])\n",
    "    scores = scores + mask\n",
    "    \n",
    "    attention_weights = jax.nn.softmax(scores, axis=-1)\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test masked attention\n",
    "output, weights = masked_self_attention(query, key, value)\n",
    "plot_attention(weights[0], \"Masked Attention Pattern\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
