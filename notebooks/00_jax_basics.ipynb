{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Fundamentals and Transformations\n",
    "\n",
    "This notebook introduces the fundamental concepts of JAX and its key transformations. We'll cover:\n",
    "1. Automatic differentiation (grad)\n",
    "2. Just-In-Time compilation (jit)\n",
    "3. Vectorization (vmap)\n",
    "4. Parallel computation (pmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic Differentiation\n",
    "\n",
    "JAX makes it easy to compute gradients of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simple_nn(params, x):\n",
    "    \"\"\"A simple neural network forward pass.\"\"\"\n",
    "    w1, b1, w2, b2 = params\n",
    "    h1 = jax.nn.relu(jnp.dot(x, w1) + b1)\n",
    "    return jnp.dot(h1, w2) + b2\n",
    "\n",
    "# Initialize parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "w1 = jax.random.normal(key, (2, 3))\n",
    "b1 = jax.random.normal(key, (3,))\n",
    "w2 = jax.random.normal(key, (3, 1))\n",
    "b2 = jax.random.normal(key, (1,))\n",
    "params = (w1, b1, w2, b2)\n",
    "\n",
    "# Compute gradients\n",
    "grad_fn = jax.grad(lambda p, x: jnp.sum(simple_nn(p, x)))\n",
    "x = jnp.array([[1.0, 2.0]])\n",
    "grads = grad_fn(params, x)\n",
    "\n",
    "print(\"Gradients of first layer weights:\")\n",
    "print(grads[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Just-In-Time Compilation\n",
    "\n",
    "JIT compilation can significantly speed up your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a computation-heavy function\n",
    "def slow_function(x):\n",
    "    return jnp.sum(jnp.sin(x) ** 2 + jnp.cos(x) ** 2)\n",
    "\n",
    "# Create a JIT-compiled version\n",
    "fast_function = jax.jit(slow_function)\n",
    "\n",
    "# Compare performance\n",
    "x = jax.random.normal(key, (1000, 1000))\n",
    "\n",
    "# Warm-up\n",
    "_ = slow_function(x)\n",
    "_ = fast_function(x)\n",
    "\n",
    "# Time comparison\n",
    "start = time.time()\n",
    "_ = slow_function(x)\n",
    "print(f\"Regular: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "_ = fast_function(x)\n",
    "print(f\"JIT: {time.time() - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorization with vmap\n",
    "\n",
    "vmap allows you to vectorize functions that operate on single examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def single_example_fn(x):\n",
    "    \"\"\"Function that operates on a single example.\"\"\"\n",
    "    return jnp.sin(x) ** 2\n",
    "\n",
    "# Create vectorized version\n",
    "batch_fn = jax.vmap(single_example_fn)\n",
    "\n",
    "# Test on batch of inputs\n",
    "x_batch = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "result = batch_fn(x_batch)\n",
    "print(\"Vectorized result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Computation with pmap\n",
    "\n",
    "pmap enables parallel computation across multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Only runs if multiple devices are available\n",
    "if len(jax.devices()) > 1:\n",
    "    def parallel_fn(x):\n",
    "        return jnp.sum(jnp.sin(x) ** 2)\n",
    "\n",
    "    # Create parallel version\n",
    "    parallel_mapped_fn = jax.pmap(parallel_fn)\n",
    "\n",
    "    # Create data for each device\n",
    "    n_devices = len(jax.devices())\n",
    "    x_parallel = jax.random.normal(key, (n_devices, 1000))\n",
    "\n",
    "    result = parallel_mapped_fn(x_parallel)\n",
    "    print(\"Parallel computation result:\", result)\n",
    "else:\n",
    "    print(\"This example requires multiple devices to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining Transformations\n",
    "\n",
    "You can combine JAX transformations for powerful effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a function that computes gradients for a batch\n",
    "@jax.jit  # Make it fast\n",
    "@jax.vmap  # Vectorize it\n",
    "def batch_gradients(x):\n",
    "    return jax.grad(lambda x: jnp.sum(jnp.sin(x) ** 2))(x)\n",
    "\n",
    "# Test it\n",
    "x_batch = jax.random.normal(key, (10, 5))\n",
    "grads = batch_gradients(x_batch)\n",
    "print(\"Combined transformation result shape:\", grads.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
